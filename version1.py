# ğŸ”Š SENA - ì•ˆì •í™” ë²„íŠ¼ ê¸°ë°˜ ë…¹ìŒ + ëŒ€í™” ë¡œê·¸
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from TTS.api import TTS
import sounddevice as sd
import numpy as np
import tempfile
import soundfile as sf

# -------------------------------
# ëª¨ë¸ & TTS ë¡œë“œ
# -------------------------------
MODEL_NAME = "meta-llama/Llama-3.2-1B"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

@st.cache_resource(show_spinner=True)
def load_model():
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)
    return tokenizer, model

tokenizer, model = load_model()

@st.cache_resource
def load_tts():
    tts = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC",
              progress_bar=False,
              gpu=torch.cuda.is_available())
    return tts

tts = load_tts()

# -------------------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# -------------------------------
if "conversation" not in st.session_state:
    st.session_state.conversation = []

# -------------------------------
# ë…¹ìŒ í•¨ìˆ˜
# -------------------------------
def record_audio(duration=5, fs=16000):
    """duration ì´ˆ ë™ì•ˆ ë…¹ìŒí•˜ê³  ë°˜í™˜"""
    st.info(f"ğŸ”´ Listening for {duration} seconds...")
    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')
    sd.wait()
    st.success("âœ… Finished!")
    return audio.flatten(), fs

# -------------------------------
# STT
# -------------------------------
def run_stt(audio, fs):
    tmp = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    sf.write(tmp.name, audio, fs)
    import whisper
    model = whisper.load_model("base", device="cpu")
    result = model.transcribe(tmp.name, task="transcribe", language="en", temperature=0)
    return result.get("text", "").strip()

# -------------------------------
# LLM ë‹µë³€ ìƒì„±
# -------------------------------
def generate_ai_response(user_input):
    conversation_text = ""
    for turn in st.session_state.conversation[-5:]:
        conversation_text += f"Student: {turn['user']}\nTutor: {turn['ai']}\n"

    prompt = f"""
You are a friendly English conversation tutor.
Your Persona:
- You are friendly, encouraging, and patient.
- Your goal is to make the student feel comfortable and encourage them to speak more.

Conversation Rules:
1.  **Be expressive:** Your response should be around 3-4 complete sentences.
2.  **Always ask a question:** You MUST end your response with a question to keep the conversation going.
3.  **Paraphrase, Don't Repeat:** You MUST NEVER repeat the student's sentences or key phrases.
4.  **Stay in Your Role:** You are the 'Tutor'. You MUST NOT write the "Student:" part of the conversation.

---
**Examples of what to do and what NOT to do:**

**(O) Good Example - Asks a question:**
Student: "I enjoyed watching a movie yesterday."
Tutor: "That sounds like a fun way to relax! It's great to take a break sometimes. What kind of film was it?"

**(X) Bad Example - Does NOT ask a question:**
Student: "I like listening to music."
Tutor: "That's a great hobby. Music can be very relaxing."
**(This is bad because it doesn't ask a question and ends the conversation.)**
---

Conversation so far:
{conversation_text}

Student: "{user_input}"
Tutor:"""
    
    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    outputs = model.generate(
        **inputs,
        max_new_tokens=80,
        temperature=0.7,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
    )
    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Tutor: ë’¤ì— ì˜¤ëŠ” ë¶€ë¶„ë§Œ ì¶”ì¶œ
    if "Tutor:" in reply:
        reply = reply.split("Tutor:")[-1].strip()

    return reply


# -------------------------------
# TTS ì¬ìƒ
# -------------------------------
def speak(text):
    wav = tts.tts(text=text)
    sd.play(wav, samplerate=tts.synthesizer.output_sample_rate)
    sd.wait()

# -------------------------------
# Streamlit UI
# -------------------------------
st.title("ğŸ”Š SENA - English Tutor")
st.write("ğŸ¤ ë²„íŠ¼ì„ ëˆŒëŸ¬ ëŒ€í™”í•˜ì„¸ìš”!")

# ë…¹ìŒ ë²„íŠ¼
duration = st.slider("ë°œí™” ê¸¸ì´ (ì´ˆ)", 5, 15, 5)
if st.button("My turn"):
    audio, fs = record_audio(duration, fs=16000)
    # --- 1. ìŒì„±ì¸ì‹ ì¤‘ ìŠ¤í”¼ë„ˆ í‘œì‹œ ---
    with st.spinner(""):
        user_text = run_stt(audio, fs)
    
    if user_text:
        st.markdown(f"**You:** {user_text}")

        # --- 2. AI ë‹µë³€ ìƒì„± ì¤‘ ìŠ¤í”¼ë„ˆ í‘œì‹œ (ìš”ì²­í•˜ì‹  ë¶€ë¶„) ---
        with st.spinner(""):
            ai_reply = generate_ai_response(user_text)
            st.session_state.conversation.append({"user": user_text, "ai": ai_reply})
        
        st.markdown(f"**AI Tutor:** {ai_reply}")

        # --- 3. TTS ìŒì„± ìƒì„± ì¤‘ ìŠ¤í”¼ë„ˆ í‘œì‹œ ---
        with st.spinner(""):
            speak(ai_reply)
    else:
        st.warning("Could not recognize any speech. Please try again.")
    user_text = run_stt(audio, fs)
    st.markdown(f"**You:** {user_text}")

    ai_reply = generate_ai_response(user_text)
    st.session_state.conversation.append({"user": user_text, "ai": ai_reply})
    st.markdown(f"**AI Tutor:** {ai_reply}")
    speak(ai_reply)

# ëŒ€í™” ë¡œê·¸
st.markdown("## ğŸ’¬ ëŒ€í™” ë‚´ìš©")
for turn in st.session_state.conversation:
    st.markdown(f"**You:** {turn['user']}")
    st.markdown(f"**AI Tutor:** {turn['ai']}")
